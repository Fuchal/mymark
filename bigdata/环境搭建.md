# 大数据单机学习环境hadoop/spark安装配置

## 系统环境配置

```shell
vi ~/.zshrc

export JAVA_HOME=/data/TencentKona8
export MAVEN_HOME=/data/apache-maven
export FLINK_HOME=/data/flink
export SPARK_HOME=/data/spark
export HADOOP_HOME=/data/hadoop
export PATH=$PATH:${JAVA_HOME}/bin:${MAVEN_HOME}/bin:${HADOOP_HOME}/bin:${HADOOP_HOME}/sbin:${SPARK_HOME}/bin:${FLINK_HOME}/bin
```

## 配置hadoop

### hadoop-env.sh

```shell
vi /data/hadoop/etc/hadoop/hadoop-env.sh
# 追加java_home环境变量
export JAVA_HOME=/data/TencentKona8
```

### core-site.xml

```shell
vi /data/hadoop/etc/hadoop/core-site.xml
# 配置主机域名和端口、用户名、临时文件目录地址
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:9000</value>
    </property>
    <property>
        <name>hadoop.http.staticuser.user</name>
        <value>lighthouse</value>
    </property>
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/data/hadoop/tmp</value>
    </property>
</configuration>
```

### hdfs-site.xml

```shell
vi /data/hadoop/etc/hadoop/hdfs-site.xml
# 配置hdfs副本数量、启用hdfs webui
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
    <property>
        <name>dfs.webhdfs.enabled</name>
        <value>true</value>
    </property>
</configuration>
```

### mapred-site.xml

```shell
vi /data/hadoop/etc/hadoop/mapred-site.xml
# 使用yarn来管理mr任务
<configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
</configuration>
```

### yarn-site.xml

```shell
vi /data/hadoop/etc/hadoop/yarn-site.xml
# 配置yarn 启用mr任务、yarn资源管理器webui端口、虚拟内存与物理比率
<configuration>
<!-- Site specific YARN configuration properties -->
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <property>
        <name>yarn.resourcemanager.webapp.address</name>
        <value>${yarn.resourcemanager.hostname}:18088</value>
    </property>
    <property>
        <name>yarn.nodemanager.vmem-pmem-ratio</name>
        <value>4</value>
    </property>
</configuration>
```

## spark on yarn 配置

### spark-env.sh

```shell
# 进入spark/conf目录
cp spark-env.sh.template spark-env.sh
vi spark-env.sh
# 添加一些额外的配置
# - HADOOP_CONF_DIR, to point Spark towards Hadoop configuration files
HADOOP_CONF_DIR=/data/hadoop/etc/hadoop
# - YARN_CONF_DIR, to point Spark towards YARN configuration files when you use YARN
YARN_CONF_DIR=/data/hadoop/etc/hadoop
# If 'hadoop' binary is on your PATH
SPARK_DIST_CLASSPATH=$(hadoop classpath)
# 本机最多可用cpu核数
SPARK_WORKER_CORES=2
# 本机最多可用内存数据
SPARK_WORKER_MEMORY=2G
```

### spark-defaults.conf

```shell
# 这步主要是解决 Spark On Yarn报警告信息 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set,
# falling back to uploading libraries under SPARK_HOME.
# 进入spark home/conf目录，打把spark jars
jar cv0f spark-libs.jar -C $SPARK_HOME/jars/ .
# 新建hdfs路径
hdfs dfs -mkdir -p /spark/jar
# 上传jars到HDFS
hdfs dfs -put spark-libs.jar /spark/jar
# 增加 spark-defaults.conf 配置
cp spark-defaults.conf.template spark-defaults.conf
vi spark-defaults.conf
spark.yarn.archive=hdfs:///spark/jar/spark-libs.jar
# 确认spark-libs上传hdfs成功后，清理本地文件
rm spark-libs.jar
```
